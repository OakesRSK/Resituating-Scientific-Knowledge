JRG

Treating "publication" as a broad category of "making performances public," the performances of interest are empirical investigations of phenomena deemed relevant to the research problems, programs, and projects we are trying to follow and which take place in scientific laboratories and other salient semi-private or limited access local sites. Publication includes traditional institutional forms such as the peer-reviewed journal article (print and online), but also audio-visual publications such as colloquia for other scientists and conference talks, YouTube videos, as well as other online text forms such as web pages, blog and Twitter postings, lab notebooks, open peer-review commentary, press releases, and media reports.

Starting with focal papers for the research problem of inferring ancestry in population genomic human ancestry studies (Rosenberg et al), PI Griesemer will code objects of interest to our project that are represented and described in scientific publications. Preliminary codes will be based on our proposed objects of inquiry in the project: D, M, F, S, along with hypotheses and project statements as well as methods statements about the set up and performance. As we gather a corpus of coded, tagged documents to archive and study further from each case study, we will build a database of these coded documents that can become a research tool and shared resource for the STS community.

Preliminary work shows that it is feasible to code and tag PDF documents in ways that will facilitate tracking representations of objects of inquiry such as datasets or representations of models (e.g. diagrams of mechanisms and numbered equations in journal articles) through networks of documents. Bechtel and colleagues (CITES) have developed methods for tracking mechanism diagrams and similar techniques can be used in this project for non-text objects.

PI Griesemer will initially use lab personnel lists from the Graham Coop laboratory at UC Davis as well as citation patterns of Coop lab personnel who work on research problems related to population genomics of human ancestry to connect lab and focal publications in networks that he will trace as he collects and code documents. As research proceeds, new codes and tags will be developed according to the methods described in Gerson (1991, 2016).

As PI Griesemer spends time in the lab and codes publications, he will expand collection and coding activities beyond focal papers to include other focal publication types: figures, presentation slide sets, talks, software code, and social media posts. These documents will be coded and tagged using similar methods. Networks will also be traced outward from Coop lab materials and sources using analogous, but likely more "manual,” techniques for these other focal objects, e.g. citations for these other kinds of publications, but also word of mouth and online comment threads in online publications and social media. We will study, for example, whether “pers. comm.” (personal communication) citations can be tracked and tagged to usefully represent word-of-mouth networks.

As publications of these various kinds are collected and coded, and objects of interest tracked, PI Griesemer will also begin to analyze this information for the kinds of levels, mappings, and strategies detailed in Morgan (2014). In particular, her view that knowledge must sometimes be "desituated" from a local, situated context to a generic level before it can be "resituated" to another local, situated context also suggests that some modes of "travel" of publications or objects of interest map one to many, as traditional views of knowledge diffusion or dissemination propose (CITE). In other words, knowledge travels by being broadcast to many other sites simultaneously via mass distribution practices such as publication in journals to which many libraries and readers subscribe, as is inherent in the concept of "mass" media. To track resituation, however, we need to assess uptake and not only distribution of information, e.g. via subscription or download records [CITE???]. We suspect that most knowledge resituation is best understood as a local-to-local transmission process and that diffusion is a poor metaphor or model simply because scientific research uptake in a new local setting involves poorly understood social mechanisms requiring a richer analysis than simply to refer to them as "reading." [MORE ON READING? OR NOT?]

If PI Griesemer codes 1 document/day, 5 days/week, 10 weeks/quarter, for the 8 quarters of the project, he will collect a corpus of 400 documents. Adding a similar coding effort of the two post-doctoral fellows, the project would produce a corpus of coded documents of approximately 1200 items. This is large enough to be considered for topic modeling analyses (see Brett 2012). For example, Murdoch et al. (2015) analyzed a collection of 665 full-text documents read by Darwin and recorded in his “reading diaries.”

We will use topic modeling of the corpus in our database as a quantitative complement to our qualitative, grounded theory methodology. If our topic modeling yields topics related to the research products or objects of interest — datasets, models, findings, software — that are supported by our qualitative analysis from field work, we can then apply topic modeling to much larger corpora of documents that can be collected from online resources such as JStor to extend and enrich our interpretation of qualitative findings based on fieldwork.

-------
JRG:

As I see this way of framing our project, the workflows that Alok is suggesting are all about extracting information on how the gap from download to citation gets filled. What the framing adds to what we put in the original proposal is articulation with topic modeling applied to the kinds of things we want to track: datasets, models, findings, software: do these emerge as topics in the corpus of publications we collect and track along the lines we would expect from what the qualitative research reveals on these research problems, or do they run in other directions that topic modeling misses because social processes in labs, in specialties, and in research organizations operate differently than a simple model of diffusion, endorsement, trial repetition, and new resituated report on “use.” 

From this point of view, the work to be done is about tracking research products that come into and go out of the labs at the field sites we’ve selected in such a way that our findings about them can be related to what we would also track from a “digital humanities” approach. Our “materials and methods” would include the stuff on the various research products we want to track (datasets, models, findings, software) and the methods are qualitative grounded theory to study whether this initial research product list gives us a fruitful starting point for tracking what goes on in these field sites (in the space between download and citation), so we would use grounded theory analysis of field data to develop a classification and cross-classifications of what to track and also learn how best to track from our informants. Then, the other methods would be the “quantitative” methods of topic modeling: assembling a corpus of documents (either downloads from the literature or other documents representing publications, including our own field notes where we are recording things said in labs along with records of other research products found in blog posts, tweets, presentation slide sets, and so forth). We can study small corpuses related to our field sites, but also larger ones drawn from the literatures on the research problems, projects, and specialties in which our informants are practicing.

The research question becomes much simplified in this framing: what happens in labs such that a download choice is made and how the publication travels from download to citation. Since we’ll look at this in field sites that download from other field sites in our study, we can track resituation operationally and then use our findings about pathways and mechanisms to connect to the larger research systems via digital humanities.

-------------------------
EMG: 

This could be made to make sense, but it’s a much larger project than the one originally proposed, would require a massive rewrite, and couldn’t be done by Aug. 3. I suggest we don’t try it, certainly on this round.

I think we need a conceptually smaller project (which is still probably too large): What happens when researchers look at publications?  This breaks into two overlapping questions (each quite large): (a) What difference has the emergence of new/expanded kinds of publication made to the way research is conducted?, (b) How do scientists use publications? 

The potentially interesting intellectual novelty and practical payoff come from the second question. It hasn’t been studied, and the answers (general interest to science studies aside) have all sorts of implications for research policy, training, education programs, public understanding of science, etc etc. 

Framing and justifying the core question is easy. Explaining how we’re going to get answers to it is not easy. The core methods trick is what Bruno called “follow the scientists”, which means looking at where/how/when/etc publication use (searching for pubs on particular topics, scanning the available pubs for potential new findings, tools, etc., close analysis of relevant pubs in order to aid current research, adoption and adaptation of concepts/methods/models, facts, etc from pubs, use of pubs as educational devices.  And so on.  Finding ways to recognize, record, and analyze impacts of pubs as these processes go on.

As work proceeds, all these sub-questions will get refined in the light of our work, and we;ll get a better idea of what’s going on, which is the point of the study.
